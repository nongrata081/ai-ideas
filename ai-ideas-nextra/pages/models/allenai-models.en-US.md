# AllenAI Models

## allenai 

[allenai.org/language-models](https://allenai.org/language-models)

### olmo

[olmo](https://allenai.org/olmo)
- Open Language Model (OLMo) is a framework intentionally designed to provide access to data, training code, models, and evaluation code necessary to advance AI through open research by empowering academics and researchers to study the science of language models collectively.
- Open Language Model: OLMo. A highly performant, truly open LLM and framework intentionally designed with access to the data, training code, models, and evaluation code necessary to advance AI and study language models collectively.
- What OLMo provides for researchers and developers
    - **More transparency**. With **full insight into the training data** behind the model, researchers can work more efficiently and bypass the need to rely on qualitative assumptions of model performance.
    - **Less carbon**. By opening the **full training and evaluation ecosystem**, we can radically **reduce developmental redundancies**, which is critical in the decarbonization of AI.
    - **Lasting impact**. By **keeping models** and their **datasets in the open** rather than hidden behind APIs, we **enable researchers to learn and build** from previous models and work.

- [dolma](/datasets#dolma) dataset, used for training olmo


### Tulu

https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618

---

### AllenAI Multi-modal models
https://allenai.org/multimodal-models

- [Unified-IO](https://unified-io.allenai.org/)
    - The first general-purpose neural model to perform a large and diverse set of AI tasks spanning classical computer vision, image synthesis, vision-and-language, and natural language processing (NLP).

- [Unified-IO-2](https://unified-io-2.allenai.org/)
    - The first autoregressive multimodal model capable of understanding and generating images, text, audio, and action, achieving state-of-the-art performance on the GRIT benchmark and strong results in more than 30 benchmarks.
    - [arxiv](https://arxiv.org/abs/2312.17172)
    - [paper pdf](https://arxiv.org/pdf/2312.17172)
    - [github](https://github.com/allenai/unified-io-2)

- [VisProg](https://prior.allenai.org/projects/visprog)
    - A modular and interpretable neuro-symbolic vision system that can decompose natural language instructions into a sequence of steps and then use existing pretrained neural models, image processing subroutines, or arithmetic and logical operations to execute these steps.

---

