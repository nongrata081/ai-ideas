# AI Safety

- [Managing extreme AI risks amid rapid progress](https://managing-ai-risks.com/)
    - [Paper](https://arxiv.org/pdf/2310.17688)
- [Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://www.alignmentforum.org/posts/LkECxpbjvSifPfjnb/towards-guaranteed-safe-ai-a-framework-for-ensuring-robust-1)
    - [Paper](https://arxiv.org/abs/2405.06624)


- [How to hack the simulation](https://www.researchgate.net/publication/364811408_How_to_Hack_the_Simulation)


## Importance of AI Safety

As described in [AI Safety](/ai-safety) there are many considerations about safety and predictability of AI that have to be
studied more, understood and somehow dealt with. There is some experience in the industry, as well as some best practices,
that are worth of and maybe even mandatory to be studied and possibly implemented.

One big issue in the AI field is unpredictability of neural networks behaviour. However, despite of that, there are numerous
products on the market that make use of NNs and are successful companies.

It might be a good idea to study the existing experience more in-depth, research what the possibilities are, and develop
an AI-safety policy and vision. Ideally, of course, it would be nice to have interpretability of the NNs and LLMs, and have full understanding of how things work, in order to ensure control over the technology. At the same time, an iterative approach might be a good way to resolve this task, since I assume achieving full explainability of LLMs or sufficient understanding of those, that would enable deployment of high-quality predictable results to production might take quite some time, while implementing iterations of the product with what we have now (fuzzy LLMs) allows to reach at least some visible progress right away.