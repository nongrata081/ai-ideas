# Evaluation

- [Digital Socrates: Evaluating LLMs through Explanation Critiques](https://blog.allenai.org/digital-socrates-evaluating-llms-through-explanation-critiques-12f0bed7fb7a)

---

## Cheese

- ðŸ§€ [CHEESE](https://github.com/CarperAI/cheese) Collect human annotations for your RL application with our human-in-the-loop data collection library.
    - Used for adaptive human in the loop evaluation of language and embedding models.
    - [docs](https://cheese1.readthedocs.io/en/latest/)

---

## MultiPL-E

https://github.com/nuprl/MultiPL-E

A multi-programming language benchmark for LLMs

Multi-Programming Language Evaluation of Large Language Models of Code (MultiPL-E)

MultiPL-E is a system for translating unit test-driven neural code generation benchmarks to new languages. We have used MultiPL-E to translate two popular Python benchmarks (HumanEval and MBPP) to 18 other programming languages.