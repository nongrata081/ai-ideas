# Alignment


- [alignment-research-dataset](https://github.com/moirage/alignment-research-dataset)
    - A dataset of alignment research and code to reproduce it
- arxiv [Researching Alignment Research: Unsupervised Analysis](https://arxiv.org/abs/2206.02841)

## Less wrong. Alignment Forum

- [A descriptive, not prescriptive, overview of **current AI Alignment Research**](https://www.lesswrong.com/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai)

- [lesswrong.com](https://www.lesswrong.com/about)
- [alignmentforum.org](https://www.alignmentforum.org/about?_ga=2.35807586.356722013.1725974311-294822880.1725974310)

## Hugging Face H4

- [HuggingFaceH4](https://huggingface.co/HuggingFaceH4)
    - Hugging Face H4 team, focused on aligning language models to be helpful, honest, harmless, and huggy ðŸ¤—.
- [alignment-handbook](https://github.com/huggingface/alignment-handbook)
    - Robust recipes to align language models with human and AI preferences

- [ZEPHYR: DIRECT DISTILLATION OF LM ALIGNMENT](https://arxiv.org/pdf/2310.16944)
    - paper pdf
    - [models & datasets](https://huggingface.co/collections/alignment-handbook/handbook-v01-models-and-datasets-654e424d22e6880da5ebc015)